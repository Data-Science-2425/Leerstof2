{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc18ae7c",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Momenteel hebben we gewerkt met datasets met een beperkt aantal features maar dat kan bijvoorbeeld snel oplopen tot 1000-en features.\n",
    "Denk hierbij bijvoorbeeld aan de bag-of-words methoden, of als we foto's willen bestuderen waar elke pixel drie waarden heeft (rgb).\n",
    "Dimensionality Reduction is een unsupervised learning techniek om de belangrijkste features uit de dataset te detecteren en zo het aantal benodigde features sterk te verlagen.\n",
    "Dit wordt vaak gebruikt als pre-processing step van bijvoorbeeld supervised learning technieken om het fitten van de modellen te versnellen.\n",
    "De meest gebruikte techniek binnen dit domein is PCA of Principal Component Analysis.\n",
    "Andere mogelijkheden voor dit te doen is \n",
    "* Linear Discriminant Analysis\n",
    "* Auto Encoders (Volgend jaar)\n",
    "* Missing Values Ratio\n",
    "* Low Variance Filter\n",
    "\n",
    "Waarom wordt dimensionality reduction vaak toegepast?\n",
    "* Curse of dimensionality: ML-technieken duren vaak veel langer met meer features en kunnen ook de accuraatheid verlagen omdat ze voor ruis/verwarring zorgen.\n",
    "* Rechtstreeks werken op visuele data bevat heel veel ruis, vooral bijvoorbeeld door kleine shifts in het beeld\n",
    "* Verwijderen van ruis\n",
    "* Compressie van data\n",
    "* Visualisatie van een dataset door ze te reduceren tot 2 of drie features. Dit kan namelijk getoond worden in een plot.\n",
    "\n",
    "**Merk op:** De resulterende features zijn niet kolommen uit de bestaande dataset maar kunnen eruit berekend worden door 1 of meerdere kolommen te combineren.\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "Redelijk snel een wiskundige techniek maar we gaan het vooral toepassen en focussen op wat eruit komt omdat dit niet altijd eenvoudig te interpreteren is.\n",
    "Daarom ga ik de werking van dit algoritme uitleggen aan de hand van een voorbeeld. \n",
    "De bron van dit voorbeeld vind je [hier](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4792f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1], s=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb074d8",
   "metadata": {},
   "source": [
    "Hoe zouden we nu deze hoeveelheid data kunnen reduceren?\n",
    "Dit kan bijvoorbeeld door alle punten te projecteren op de x-as.\n",
    "Hierbij laten we dus eigenlijk de y-as of tweede feature vallen.\n",
    "Het is echter eenvoudig in te zien dat je op deze manier veel informatie gaat verliezen.\n",
    "\n",
    "PCA gaat echter niet rechtstreeks features laten vallen maar gaat een ander assenstelsel definieren zodat de meeste informatie door de eerste as wordt voorgesteld.\n",
    "Dit is een beetje gelijkaardig aan de beste rechte vinden met lineaire regressie maar dit gebeurd op een wiskundig andere manier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe22cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# maak het PCA-model om van 2 features naar 2 principal components te gaan\n",
    "\n",
    "# fit op de data\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    # dit tekent de zwarte pijltjes per principal component\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d7353",
   "metadata": {},
   "source": [
    "Deze assen worden voorgesteld door vectoren (zie de matrix).\n",
    "De vectoren bepalen hoe welke features gecombineerd moeten worden en worden **principal components** genoemd.\n",
    "Op basis van de variantie is het ook duidelijk dat de eerste component veel belangrijker is dan de tweede.\n",
    "\n",
    "Als de dataset nu getekend wordt met het nieuwe assenstelsel bekomen we:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 figuren naast elkaar\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plot data (ax[0] is de eerste plot/figuur)\n",
    "ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='input')\n",
    "\n",
    "# plot principal components\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='component 1', ylabel='component 2',\n",
    "          title='principal components',\n",
    "          xlim=(-5, 5), ylim=(-3, 3.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf15cea",
   "metadata": {},
   "source": [
    "Je kan nu twee zaken opmerken.\n",
    "Ten eerste is de data veel beter verspreid over het assenstelsel.\n",
    "Dit komt omdat de schaal van de assen niet gelijk is en dus de data opengetrokken wordt.\n",
    "Veel belangrijker is echter dat als je nu de tweede component laat vallen, dat er veel minder data verloren gaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier hou ik maar 1 component over\n",
    "\n",
    "\n",
    "# inverse transform om de orange puntjes te kunnen plotten\n",
    "\n",
    "# blauwe puntjes\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2, s=5)\n",
    "\n",
    "#orange puntjes\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8, s=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec2c65",
   "metadata": {},
   "source": [
    "### PCA in computer visie\n",
    "\n",
    "Uit het vorige voorbeeld zou je misschien kunnen denken dat het voordeel van PCA toe te passen beperkt is.\n",
    "Onderstaande voorbeeld past PCA toe op beelden van van handgeschreven cijfers van 8 bij 8 pixels (64 features dus).\n",
    "PCA is 1 van de eerste methoden om te werken met beelden omdat het in staat was om de beschikbare data sterk te reduceren.\n",
    "In deze context worden de principal components ook vaak **Eigenfaces** genoemd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# moet je niet zelf kunnen\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10), s=5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4018f21",
   "metadata": {},
   "source": [
    "In dit voorbeeld hebben we de 64 features gereduceerd naar 2 wat slechts een 3% is van de beschikbare data.\n",
    "Echter maken de resulterende 2 principal components het reeds visueel mogelijk om verschillende klassen te detecteren voor classificatie van de beelden.\n",
    "\n",
    "Wat is nu de betekenis van de resulterende principal components?\n",
    "Zonder PCA is de waarde van elke pixel noodzakelijk om een accuraat beeld op te bouwen van het origineel.\n",
    "Dit wordt hieronder getoond waar door de eerste 8 pixels te gebruiken, elke de eerste rij van het beeld ingevuld wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a77f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPGELET: DE CODE OM DE FIGUREN TE MAKEN IN DEZE FUNCTIE MOET JE NIET KUNNEN REPRODUCEREN\n",
    "def plot_pca_components(x, coefficients=None, mean=0, components=None,\n",
    "                        imshape=(8, 8), n_components=8, fontsize=12,\n",
    "                        show_mean=True):\n",
    "    # validatie/fouten voorkomen\n",
    "    if coefficients is None:\n",
    "        coefficients = x\n",
    "        \n",
    "    if components is None:\n",
    "        components = np.eye(len(coefficients), len(x))\n",
    "        \n",
    "    mean = np.zeros_like(x) + mean\n",
    "        \n",
    "    # maakt de figuur aan (Grid is 2 rijen + 13 kolommen) (4 is om de eerste en laatste figuur 2 kolommen elk te laten innemen)\n",
    "    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n",
    "    g = plt.GridSpec(2, 4 + bool(show_mean) + n_components, hspace=0.3)\n",
    "\n",
    "    # feature vector to image en plot\n",
    "    def show(i, j, x, title=None):\n",
    "        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n",
    "        # imshow om figuur/matrix te tonen\n",
    "        ax.imshow(x.reshape(imshape), interpolation='nearest')\n",
    "        if title:\n",
    "            ax.set_title(title, fontsize=fontsize)\n",
    "\n",
    "    show(slice(2), slice(2), x, \"True\")\n",
    "    \n",
    "    approx = mean.copy()\n",
    "    \n",
    "    counter = 2\n",
    "    if show_mean:\n",
    "        show(0, 2, np.zeros_like(x) + mean, r'$\\mu$')\n",
    "        show(1, 2, approx, r'$1 \\cdot \\mu$')\n",
    "        counter += 1\n",
    "\n",
    "    for i in range(n_components):\n",
    "        # teken de componenten een voor 1\n",
    "        approx = approx + coefficients[i] * components[i]\n",
    "        # teken de bovenste rij\n",
    "        show(0, i + counter, components[i], r'$c_{0}$'.format(i + 1))\n",
    "        # teken de onderste rij\n",
    "        show(1, i + counter, approx,\n",
    "             r\"${0:.2f} \\cdot c_{1}$\".format(coefficients[i], i + 1))\n",
    "        if show_mean or i > 0:\n",
    "            plt.gca().text(0, 1.05, '$+$', ha='right', va='bottom',\n",
    "                           transform=plt.gca().transAxes, fontsize=fontsize)\n",
    "\n",
    "    show(slice(2), slice(-2, None), approx, \"Approx\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2a791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46221439",
   "metadata": {},
   "source": [
    "Als er nu gebruik gemaakt wordt van PCA dan leveren de eerste 8 principal components een heel ander beeld op.\n",
    "Meer zelfs, we bekomen reeds een goede benadering van het origineel na het gebruik van 3 features ipv 64 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a025864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf3bf72",
   "metadata": {},
   "source": [
    "### Aantal principal components nodig\n",
    "\n",
    "De belangrijkste hyperparameter die nodig is voor het toepassen van PCA is het aantal componenten die er gebruikt worden. \n",
    "Dit kan net zoals bij clustering door gebruik te maken van de elbow-methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data) # n_components = aantal features origineel\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# explained variance => abosulte waarde van variance dat per component uitgelegd wordt\n",
    "# explained variance ration => percentage van variance dat per component uitgelegd wordt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce6c82",
   "metadata": {},
   "source": [
    "In deze figuur valt op dat met reeds 10 componenten reeds 75% van de variantie bereikt wordt (en 20 componenten bevatten 90% van de informatie).\n",
    "Dit geeft dus weer dat het aantal features in de oorspronkelijke dataset sterk gereduceerd kan worden zonder veel informatie te verliezen.\n",
    "\n",
    "### Noise filtering\n",
    "\n",
    "Er werd aangehaald dat PCA ook gebruikt kan worden om ruis te verwijderen uit beelden of geluid. \n",
    "In het onderstaande wordt hiervan een voorbeeld uitgewerkt.\n",
    "Eerst wordt de originele data zonder en met ruis getoond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8,8), cmap='binary', interpolation='nearest', clim=(0,16))\n",
    "\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(digits.data, 40) # 4 bepaalt hoe sterk de ruis is\n",
    "\n",
    "plot_digits(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605aca5b",
   "metadata": {},
   "source": [
    "Nu kan er een PCA-model getrained worden op de noisy-data waarbij er bijvoorbeeld gewenst is dat er nog 50% van de informatie behouden blijft.\n",
    "Het valt op dat de ruis sterk verminderd is na het uitvoeren van PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.5) # hiermee gaat hij zelf bepalen hoeveel componenten hij nodig heeft om 50% informatie over te houden\n",
    "denoised_pca = pca.fit_transform(noise)\n",
    "\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddff890",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = pca.inverse_transform(denoised_pca) # ga terug van 12 naar 64 features\n",
    "plot_digits(denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d670b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
