{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9acf77",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "De **supervised learning** techniek Decision Trees voor **classificatie** is iets dat je zelf al goed kent.\n",
    "Je gebruikt ze namelijk zelf onbewust wanneer je een voorwerp moet identificeren.\n",
    "Waar let je bijvoorbeeld op om het onderstaande stuk fruit te identificeren?\n",
    "\n",
    "![apple](images\\apple.png)\n",
    "\n",
    "Hier zou je bijvoorbeeld op de volgende karakteristieken of features gelet kunnen hebben:\n",
    "* Hoe rood is het stuk fruit?\n",
    "* Hoe rond is het stuk fruit\n",
    "* Hoeveel weegt het stuk fruit?\n",
    "\n",
    "Mentaal bouw je dus onbewust een boom op en overloop je een reeks opties om te bepalen of het object waar je naar kijkt een appel is of niet.\n",
    "\n",
    "Dit is net hetgene dat er ook gebruikt bij het opbouwen van een decision tree.\n",
    "Een voorbeeld van de werking van deze classifier is als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e18fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# graphical\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, class_sep=0.3, random_state=123)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:, 1], hue=y)\n",
    "plt.plot([0.45, 0.45], [-1.5, 1.2], color=\"black\")\n",
    "plt.plot([-1.2, 0.45], [-0.5, -0.5], color=\"black\")\n",
    "plt.plot([-1.2, 0.45], [1.0, 1.0], color=\"black\")\n",
    "plt.plot([-1.2, 0.45], [0.1, 0.1], color=\"black\")\n",
    "plt.plot([-0.55, -0.55], [0.1, 1.0], color=\"black\")\n",
    "plt.plot([0, 0], [0.1, -0.5], color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94da7f",
   "metadata": {},
   "source": [
    "Voor bovenstaande voorbeeld kan bijvoorbeeld deze boom opgebouwd worden die de classificatie doet:\n",
    "\n",
    "![boom](images\\boom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf61b18",
   "metadata": {},
   "source": [
    "## Bepalen scheidingslijnen\n",
    "\n",
    "Hierboven hebben we zelf de lijnen getrokken om het gebied in twee te verdelen.\n",
    "Nu rest er nog de vraag, hoe kiezen we de beste lijnen om deze verdeling te doen.\n",
    "Intuitief denk je dan aan de lijn die het grootste aantal observaties van dezelfde klasse afsplitst. \n",
    "Of de lijn die het minste fouten maakt.\n",
    "\n",
    "Deze intuitieve benadering komt sterk overeen met het berekenen van de entropie en deze proberen zo laag mogelijk te krijgen.\n",
    "Entropie is een maat voor de wanorde/chaos in een systeem en wordt gebruikt in een aantal wetenschappelijke disciplines zoals chemie en fysica maar ook in computerwetenschappen bij bijvoorbeeld encryptie en compressie. \n",
    "Hoe hoger de entropie in een bepaald systeem, hoe hoger de wanorde/chaos en bijvoorbeeld hoe beter de encryptie.\n",
    "De entropie van een classificatieprobleem kan als volgt bepaald worden:\n",
    "\n",
    "$H = \\sum\\limits_{i=1}^{N} p_i \\log_2(\\frac{1}{p_i}) = -\\sum\\limits_{i=1}^{N} p_i \\log_2(p_i)$\n",
    "\n",
    "waar $p_i$ het percentage observaties van klasse $i$ is in het gebied.\n",
    "\n",
    "De entropie van de beginsituatie in bovenstaand voorbeeld en na de eerste split kan berekend worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pBeginKlasse0 = 9/19 # blauwe\n",
    "pBeginKlasse1 = 10/19 # orange\n",
    "\n",
    "entropieBegin = - (pBeginKlasse0 * np.log2(pBeginKlasse0) + pBeginKlasse1 * np.log2(pBeginKlasse1))\n",
    "print(pBeginKlasse0, pBeginKlasse1, entropieBegin)\n",
    "# Dit geeft een zeer hoge entropie voor in het begin.\n",
    "\n",
    "pLinksKlasse0 = 9/13\n",
    "pLinksKlasse1 = 4/13\n",
    "pRechtsKlasse0 = 0.0000001 # Dit moet 0 zijn maar zou eindigen in min oneindig voor de log. (vandaar een minieme waarde)\n",
    "pRechtsKlasse1 = 6/6\n",
    "\n",
    "entropieRechts = - (pRechtsKlasse0 * np.log2(pRechtsKlasse0) + pRechtsKlasse1 * np.log2(pRechtsKlasse1))\n",
    "print(pRechtsKlasse0, pRechtsKlasse1, entropieRechts)\n",
    "entropieLinks = - (pLinksKlasse0 * np.log2(pLinksKlasse0) + pLinksKlasse1 * np.log2(pLinksKlasse1))\n",
    "print(pLinksKlasse0, pLinksKlasse1, entropieLinks)\n",
    "informationGain = entropieBegin - 6/19 * entropieRechts - 13/19 * entropieLinks\n",
    "print(\"Information Gain = \", informationGain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c867e0",
   "metadata": {},
   "source": [
    "Na het berekenen van de gewenste information gain voor alle splits die je kan doen, moet je de beste kiezen. \n",
    "Hiervoor kiezen we dan degene die de grootste hoeveelheid informatie toevoegt.\n",
    "\n",
    "Er is echter een groot nadeel aan het gebruik van de entropie.\n",
    "Het veelvuldig berekenen van de logaritmes voor alle mogelijke verdeling is zeer rekenintensief.\n",
    "Daarom is het beter om gebruik te maken van een alternatieve methode om de scheidingslijn te bepalen.\n",
    "Deze methode noemt de Gini impurity methode en is default gekozen bij de Decision Tree algoritmes van sklearn.\n",
    "De Gini impurity focust meer op de zuiverheid van de verdeling en minder op de entropie en kan berekend worden als volgt:\n",
    "\n",
    "$G = 1 - \\sum\\limits_{i=1}^{N}p_i^2$\n",
    "\n",
    "Om nu de beste verdelingslijn te zoeken neem je de lijn die leidt tot de kleinste Gini impurity.\n",
    "Let er wel op dat je het gewogen gemiddelde neemt van de twee delen om de Gini impurity waarden te vergelijken.\n",
    "De berekening van de Gini Impurity voor de eerste verdeling van bovenstaande voorbeeld gaat als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLinks = 1 - pLinksKlasse0**2 - pLinksKlasse1**2\n",
    "GRechts = 1 - pRechtsKlasse0**2 - pRechtsKlasse1**2\n",
    "G = 13/19*GLinks + 6/19*GRechts\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cdc4f",
   "metadata": {},
   "source": [
    "Net zoals bij de voorgaande machine learning technieken hebben ook Decision Trees hun voor en nadelen.\n",
    "De nadelen zijn:\n",
    "* Grote neiging tot overfitting\n",
    "* Gevoelig aan ruis\n",
    "* Boom kan zeer groot worden\n",
    "\n",
    "Het vordeel van een decision tree is dat het een zeer snelle en eenvoudige techniek is die snel een resultaat kan leveren maar het belangrijkste voordeel waardoor het nog steeds vaak gebruikt wordt is dat je op het resulterende model kan redeneren.\n",
    "Dat wil zeggen dat je kan kijken welke features de grootste impact hebben op het resultaat, waar de verdelingslijnen geplaatst worden en waarom. \n",
    "Deze redenering en analyse kan dan gebruikt worden om je processen eventueel te verbeteren. \n",
    "\n",
    "Hoewel Decision Trees gevoelig zijn aan overfitting zijn er een groot aantal manieren om regularisatie toe te voegen aan het algoritme.\n",
    "De belangrijkste hyperparameters om dit te doen zijn:\n",
    "* max_depth: Beperk de maximale diepte van de boom\n",
    "* min_samples_split: Laat geen verdelingen toe van gebieden met minder observaties dan deze waarde. De klasse van het gebied komt dan overeen met de meerderheid van de observaties\n",
    "* min_samples_leaf: Minimum aantal observaties als blad van de boom\n",
    "\n",
    "Train nu een [decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) op de borstkanker dataset van sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9216d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
