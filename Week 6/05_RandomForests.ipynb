{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f50aaa",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Een andere manier om overfitting van decision trees tegen te gaan en het resultaat accurater te maken is door gebruik te maken van Random Forests.\n",
    "Deze techniek gaat meerdere decision trees trainen en door gebruik van majority voting alle trees laten stemmen over het uiteindelijke resultaat.\n",
    "Buiten het vermijden van problemen door overfitting kan deze techniek ook veel accurater zijn.\n",
    "Algemeen gezien kan men zelfs er van uitgaan dat hoe meer trees er gebruikt worden hoe accurater het forest wordt.\n",
    "\n",
    "Omdat we meerdere bomen gebruiken willen we ook meer variatie in de modellen die deze bomen leren.\n",
    "Daarom wordt elke boom maar op een willekeurig deel van de data getrained (vandaar de term random).\n",
    "\n",
    "Buiten de hyperparameters die voor de decision trees gebruikt kunnen worden zijn er nog een aantal extra zaken die de performantie van het gecombineerde forest sterk kan verbeteren, namelijk:\n",
    "* n_estimators: Het aantal bomen dat gebruikt wordt (Hoe meer bomen hoe accurater maar ook hoe meer rekenkracht)\n",
    "* max_features: Aantal features per boom (int, float, sqrt / auto, log2, default). Hoe meer features, hoe meer kans op overfitting\n",
    "* Bootstrap aggregating of bagging: Dit houdt in dat maar een deel van de observaties gebruikt worden om elke boom te trainen. Zo kan er meer variatie geintroduceerd worden.\n",
    "* oob_score: Out-of-Bag score: Het trainen van een decision tree gebruikt niet alle trainingsdata dus kan je de niet-gebruikte data gebruiken als validatieset voor die boom. Meer informatie vind je [hier](https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710)\n",
    "\n",
    "Waarom zou je een Random Forest Classifier verkiezen boven een classifier gebaseerd op Logistic Regression of SVM?\n",
    "Random forest classifiers hebben de volgende voordelen:\n",
    "* Ze zijn zeer goed in te stellen\n",
    "* Ze vragen niet veel rekenkracht\n",
    "* Hun accuraatheid is gelijk aan dat van SVM of zelfs beter\n",
    "* Het voordeel van decision trees dat er kan geredeneerd worden op welke features belangrijk zijn blijft behouden. \n",
    "\n",
    "Net zoals voor de vorige classifiers kan ook deze techniek gebruikt worden voor regressie.\n",
    "In dit geval moet er gebruik gemaakt worden van het gemiddelde in plaats van majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa7cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train 0.9956043956043956\n",
      "Accuracy test 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "data = datasets.load_breast_cancer(as_frame=True)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=1000, max_depth=5, max_features='log2') # 1000 bomen, max_depth=5, log2 = hoeveel features worden aan elke boom gegeven\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy train {forest.score(X_train, y_train)}\") # logisch dat dit resultaat 1 is want zonder restricties van hyperparameter kan elk datapunt apart zitten\n",
    "print(f\"Accuracy test {forest.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ee9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04710314 0.01078635 0.05660166 0.04803576 0.00760219 0.01516445\n",
      " 0.0475745  0.09124856 0.00349961 0.00287319 0.01855576 0.00294782\n",
      " 0.01327646 0.04174234 0.00412213 0.00383843 0.00777972 0.00464335\n",
      " 0.00398511 0.00489493 0.12295544 0.01369939 0.08945848 0.12941518\n",
      " 0.0134209  0.01581848 0.04106451 0.11870823 0.0130317  0.00615224]\n",
      "Most important feature worst area\n"
     ]
    }
   ],
   "source": [
    "print(forest.feature_importances_) # hoe belangrijk is elke feature/input voor het eindresultaat\n",
    "\n",
    "for idx, val in enumerate(forest.feature_importances_):\n",
    "    if val == np.max(forest.feature_importances_):\n",
    "        print(f\"Most important feature {data.data.columns[idx]}\")\n",
    "\n",
    "# Het valt op dat hier minder nullen aanwezig zijn dan bij decision tree\n",
    "# Dit komt omdat er 1000 bomen zijn en niet elke boom heeft alle features -> hierdoor komt elke kolom wel eens aan bod om te splitsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46ae2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
